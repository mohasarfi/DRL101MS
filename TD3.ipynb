{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(state_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, action_dim)\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        \n",
    "    def forward(self, state):\n",
    "        a = F.relu(self.l1(state))\n",
    "        a = F.relu(self.l2(a))\n",
    "        return self.max_action * torch.tanh(self.l3(a))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, 1)\n",
    "        \n",
    "        # Q2 architecture\n",
    "        self.l4 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.l5 = nn.Linear(256, 256)\n",
    "        self.l6 = nn.Linear(256, 1)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], 1)\n",
    "        \n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "        \n",
    "        q2 = F.relu(self.l4(sa))\n",
    "        q2 = F.relu(self.l5(q2))\n",
    "        q2 = self.l6(q2)\n",
    "        \n",
    "        return q1, q2\n",
    "    \n",
    "    def Q1(self, state, action):\n",
    "        sa = torch.cat([state, action], 1)\n",
    "        \n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "        \n",
    "        return q1\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=1000000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward.reshape(-1, 1), next_state, done.reshape(-1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class TD3:\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(self.device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(self.device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "        \n",
    "        self.critic = Critic(state_dim, action_dim).to(self.device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(self.device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        \n",
    "        self.batch_size = 256\n",
    "        self.discount = 0.99\n",
    "        self.tau = 0.005\n",
    "        self.policy_noise = 0.2\n",
    "        self.noise_clip = 0.5\n",
    "        self.policy_freq = 2\n",
    "        \n",
    "        self.total_it = 0\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(self.device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "    \n",
    "    def train(self):\n",
    "        self.total_it += 1\n",
    "        \n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample from replay buffer\n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        action = torch.FloatTensor(action).to(self.device)\n",
    "        reward = torch.FloatTensor(reward).to(self.device)\n",
    "        next_state = torch.FloatTensor(next_state).to(self.device)\n",
    "        done = torch.FloatTensor(done).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Select action according to policy and add clipped noise\n",
    "            noise = (\n",
    "                torch.randn_like(action) * self.policy_noise\n",
    "            ).clamp(-self.noise_clip, self.noise_clip)\n",
    "            \n",
    "            next_action = (\n",
    "                self.actor_target(next_state) + noise\n",
    "            ).clamp(-self.max_action, self.max_action)\n",
    "            \n",
    "            # Compute the target Q value\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + (1 - done) * self.discount * target_Q\n",
    "            \n",
    "        # Get current Q estimates\n",
    "        current_Q1, current_Q2 = self.critic(state, action)\n",
    "        \n",
    "        # Compute critic loss\n",
    "        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "        \n",
    "        # Optimize the critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Delayed policy updates\n",
    "        if self.total_it % self.policy_freq == 0:\n",
    "            # Compute actor loss\n",
    "            actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "            \n",
    "            # Optimize the actor\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            \n",
    "            # Update the frozen target models\n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "                \n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "def train_td3(env_name=\"HalfCheetah-v4\", max_timesteps=1000000):\n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    max_action = float(env.action_space.high[0])\n",
    "    \n",
    "    agent = TD3(state_dim, action_dim, max_action)\n",
    "    \n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num = 0\n",
    "    \n",
    "    for t in range(max_timesteps):\n",
    "        episode_timesteps += 1\n",
    "        \n",
    "        # Select action and add exploration noise\n",
    "        action = (\n",
    "            agent.select_action(np.array(state))\n",
    "            + np.random.normal(0, max_action * 0.1, size=action_dim)\n",
    "        ).clip(-max_action, max_action)\n",
    "        \n",
    "        # Take action in environment\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Store data in replay buffer\n",
    "        agent.replay_buffer.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Train agent\n",
    "        agent.train()\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            print(f\"Episode {episode_num+1}: Reward = {episode_reward}, Steps = {episode_timesteps}\")\n",
    "            \n",
    "            state, _ = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            episode_timesteps = 0\n",
    "            episode_num += 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_td3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import cv2\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.array(state), action, reward, np.array(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class AtariPreprocessor:\n",
    "    def __init__(self, shape=(84, 84)):\n",
    "        self.shape = shape\n",
    "        self.frame_stack = deque(maxlen=4)\n",
    "    \n",
    "    def preprocess(self, frame):\n",
    "        # Convert to grayscale and resize\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        resized = cv2.resize(gray, self.shape, interpolation=cv2.INTER_AREA)\n",
    "        return resized / 255.0\n",
    "    \n",
    "    def reset(self, frame):\n",
    "        processed = self.preprocess(frame)\n",
    "        for _ in range(4):\n",
    "            self.frame_stack.append(processed)\n",
    "        return np.stack(self.frame_stack, axis=0)\n",
    "    \n",
    "    def step(self, frame):\n",
    "        processed = self.preprocess(frame)\n",
    "        self.frame_stack.append(processed)\n",
    "        return np.stack(self.frame_stack, axis=0)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_shape, n_actions, device=\"cuda\"):\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.policy_net = DQN(input_shape, n_actions).to(self.device)\n",
    "        self.target_net = DQN(input_shape, n_actions).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.00025)\n",
    "        self.memory = ReplayBuffer()\n",
    "        \n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "        self.eps_start = 1.0\n",
    "        self.eps_end = 0.02\n",
    "        self.eps_decay = 1000000\n",
    "        self.target_update = 10000\n",
    "        self.steps_done = 0\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * \\\n",
    "            np.exp(-1. * self.steps_done / self.eps_decay)\n",
    "        self.steps_done += 1\n",
    "        \n",
    "        if random.random() > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                q_values = self.policy_net(state)\n",
    "                return q_values.max(1)[1].item()\n",
    "        else:\n",
    "            return random.randrange(self.n_actions)\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.BoolTensor(dones).to(self.device)\n",
    "        \n",
    "        current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_states).max(1)[0]\n",
    "            next_q_values[dones] = 0.0\n",
    "            target_q_values = rewards + self.gamma * next_q_values\n",
    "        \n",
    "        loss = nn.functional.smooth_l1_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network\n",
    "        if self.steps_done % self.target_update == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "def train_dqn(env_name='PongNoFrameskip-v4', num_episodes=1000):\n",
    "    env = gym.make(env_name)\n",
    "    preprocessor = AtariPreprocessor()\n",
    "    \n",
    "    input_shape = (4, 84, 84)  # 4 stacked frames\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    agent = DQNAgent(input_shape, n_actions)\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = preprocessor.reset(state)\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            if not done:\n",
    "                next_state = preprocessor.step(next_state)\n",
    "            else:\n",
    "                next_state = None\n",
    "            \n",
    "            # Store transition in memory\n",
    "            agent.memory.push(state, action, reward, \n",
    "                            next_state if next_state is not None else np.zeros_like(state), \n",
    "                            done)\n",
    "            \n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Optimize model\n",
    "            agent.optimize_model()\n",
    "            \n",
    "            if done:\n",
    "                episode_rewards.append(episode_reward)\n",
    "                print(f\"Episode {episode + 1}/{num_episodes}, Reward: {episode_reward}\")\n",
    "                break\n",
    "        \n",
    "        # Save model periodically\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            torch.save({\n",
    "                'episode': episode,\n",
    "                'model_state_dict': agent.policy_net.state_dict(),\n",
    "                'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "                'episode_rewards': episode_rewards,\n",
    "            }, f'dqn_pong_checkpoint_{episode+1}.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_dqn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRL101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
