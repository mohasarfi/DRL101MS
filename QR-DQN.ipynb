{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "n_quantiles = 3 \n",
    "\n",
    "taus = torch.linspace(0.0, 1.0, steps=n_quantiles + 2)[1:-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2500, 0.5000, 0.7500])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "preds = torch.tensor([\n",
    "    [[1, 2, 4], [ 2, 3, 4], [1, 3, 5], [1, 2, 3]],\n",
    "    [[2, 3, 4], [-1, 0, 1], [1, 3, 5], [1, 2, 5]]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(preds.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2500, 0.5000, 0.7500]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m taus \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlinspace(\u001b[39m0.0\u001b[39m, \u001b[39m1.0\u001b[39m, steps\u001b[39m=\u001b[39mn_quantiles \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m)[\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m td_error \u001b[39m=\u001b[39m [[[ \u001b[39m2.9700\u001b[39m,  \u001b[39m1.9600\u001b[39m,  \u001b[39m1.9400\u001b[39m]],[[ \u001b[39m1.0000\u001b[39m,  \u001b[39m0.0000\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m3.0000\u001b[39m]]]\n\u001b[0;32m----> 3\u001b[0m quantile_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mabs(taus \u001b[39m-\u001b[39m (td_error \u001b[39m<\u001b[39;49m \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39mfloat())\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "taus = torch.linspace(0.0, 1.0, steps=n_quantiles + 2)[1:-1].unsqueeze(0).unsqueeze(0)\n",
    "td_error = [[[ 2.9700,  1.9600,  1.9400]],[[ 1.0000,  0.0000, -3.0000]]]\n",
    "quantile_loss = torch.abs(taus - (td_error < 0).float())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is based on the tutorial give in https://www.kaggle.com/code/auxeno/quantile-regression-dqn-rl \n",
    "\n",
    "Some graphs and ideas should be included form the following link\n",
    "\n",
    "\n",
    "Finally materials should be combined and written in the repo style to use advantages of both links! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np \n",
    "\n",
    "class QRNetwrok(nn.modules):\n",
    "    def __init__(self,observation_space,action_space,hidden_size = 64, num_quantiles = 16):\n",
    "        self.input_size = np.prod(observation_space.shape)\n",
    "        self.n_actions = action_space.n \n",
    "        self.n_quantiles = num_quantiles\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.input = nn.Linear(self.input_size , self.hidden_size)\n",
    "        self.hidden = nn.Linear(self.hidden_size,self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size , self.n_actions*self.n_quantiles)  \n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.input(x))\n",
    "        x = F.relu(self.hidden(x))\n",
    "        out = self.out(x)\n",
    "\n",
    "        return out.view(-1,self.n_actions,self.n_quantiles)\n",
    "\n",
    "# Action selection \n",
    "\n",
    "    def action_selection(self,observation , eps):\n",
    "        if np.random.rand() < eps:\n",
    "            return np.random.randint(0,self.n_actions)\n",
    "        output = self.forward(torch.tensor(observation))\n",
    "        return (output.mean(-1).argmax().item())\n",
    "from collections import deque    \n",
    "class ReplayBuffer:\n",
    "    def __init__(self,maxlen):\n",
    "        self.buffer = deque(maxlen)\n",
    "    \n",
    "    def put(self,experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self,batch_size):\n",
    "        states , actions, rewards, next_states, dones = zip(*random.sample(self.buffer,batch_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "\n",
    "class QRDQN():\n",
    "\n",
    "    def __init__(self,config):\n",
    "        self.config = config \n",
    "\n",
    "    def set_env(self,env):\n",
    "        self.env = env \n",
    "\n",
    "    def initialize_agent(self):\n",
    "\n",
    "        self.action_space = self.env.action_space  \n",
    "        self.observation = self.env.observation_space \n",
    "\n",
    "        device = self.config['device'] \n",
    "        #def __init__(self,observation_space,action_space,hidden_size = 64, num_quantiles = 16)\n",
    "\n",
    "        self.online_network = QRNetwrok(self.observation , \n",
    "                                        self.action_space, \n",
    "                                        self.config['hidden_size'], \n",
    "                                        self.config[n_quantiles]).to(device)\n",
    "        \n",
    "        self.target_network = QRNetwrok(self.observation , \n",
    "                                        self.action_space, \n",
    "                                        self.config['hidden_size'], \n",
    "                                        self.config[n_quantiles]).to(device)\n",
    "        \n",
    "        self.update_target_network(1.)\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(self.online_network.parameter() ,lr = self.config['lr'] )\n",
    "\n",
    "\n",
    "    def update_target_network(self,tau):\n",
    "        with torch.no_grad():\n",
    "            for target_params,online_params in zip(self.target_network.parameters()\n",
    "                                                , self.online_network.parameters()):\n",
    "                target_params.data.copy(tau*online_params.data + (1-tau)*target_params.data)\n",
    "\n",
    "    def selection_action(self , observation, eps):\n",
    "        if np.random.rand() < eps :\n",
    "            #return np.random.randint(self.action_space.n)\n",
    "            return self.env.action_space.sample()\n",
    "        else : \n",
    "            output = self.online_network.forward(torch.tensor(observation))\n",
    "            return output.mean(-1).argmax().index()\n",
    "    \n",
    "    def calc_exploration_rate(self,step,total_steps):\n",
    "        epsilon_start = self.config['exploration_strat_eps']\n",
    "        epsilon_end = self.config['exploration_end_eps']\n",
    "        epsilon_decay_steps = total_steps * self.config['exploration_fraction']\n",
    "\n",
    "        return max(epsilon_end ,\n",
    "                   epsilon_start - (epsilon_start - epsilon_end) * (step / epsilon_decay_steps))\n",
    "    \n",
    "    def train(self, total_steps):\n",
    "\n",
    "        episode_count = 0 \n",
    "        episodic_reward = 0 \n",
    "        episode_rewards = [np.NaN]\n",
    "\n",
    "        observation , _ = self.env.reset()\n",
    "\n",
    "        for step in (total_steps+1):\n",
    "            \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6601733860110626"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc49bab7f3878ba80ad2a65c2525cdc9d74c5f54777f705d73262dece5a900f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
