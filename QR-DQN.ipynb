{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "n_quantiles = 3 \n",
    "\n",
    "taus = torch.linspace(0.0, 1.0, steps=n_quantiles + 2)[1:-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2500, 0.5000, 0.7500])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "preds = torch.tensor([\n",
    "    [[1, 2, 4], [ 2, 3, 4], [1, 3, 5], [1, 2, 3]],\n",
    "    [[2, 3, 4], [-1, 0, 1], [1, 3, 5], [1, 2, 5]]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(preds.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2500, 0.5000, 0.7500]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m taus \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlinspace(\u001b[39m0.0\u001b[39m, \u001b[39m1.0\u001b[39m, steps\u001b[39m=\u001b[39mn_quantiles \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m)[\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m td_error \u001b[39m=\u001b[39m [[[ \u001b[39m2.9700\u001b[39m,  \u001b[39m1.9600\u001b[39m,  \u001b[39m1.9400\u001b[39m]],[[ \u001b[39m1.0000\u001b[39m,  \u001b[39m0.0000\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m3.0000\u001b[39m]]]\n\u001b[0;32m----> 3\u001b[0m quantile_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mabs(taus \u001b[39m-\u001b[39m (td_error \u001b[39m<\u001b[39;49m \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39mfloat())\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "taus = torch.linspace(0.0, 1.0, steps=n_quantiles + 2)[1:-1].unsqueeze(0).unsqueeze(0)\n",
    "td_error = [[[ 2.9700,  1.9600,  1.9400]],[[ 1.0000,  0.0000, -3.0000]]]\n",
    "quantile_loss = torch.abs(taus - (td_error < 0).float())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is based on the tutorial give in https://www.kaggle.com/code/auxeno/quantile-regression-dqn-rl \n",
    "\n",
    "Some graphs and ideas should be included form the following link\n",
    "\n",
    "\n",
    "Finally materials should be combined and written in the repo style to use advantages of both links! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np \n",
    "from collections import deque    \n",
    "\n",
    "class QRNetwrok(nn.Module):\n",
    "    def __init__(self,observation_space,action_space,hidden_size = 64, num_quantiles = 16):\n",
    "        super().__init__()\n",
    "        self.input_size = np.prod(observation_space.shape)\n",
    "        self.n_actions = action_space.n \n",
    "        self.n_quantiles = num_quantiles\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.input = nn.Linear(self.input_size , self.hidden_size)\n",
    "        self.hidden = nn.Linear(self.hidden_size,self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size , self.n_actions*self.n_quantiles)  \n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.input(x))\n",
    "        x = F.relu(self.hidden(x))\n",
    "        out = self.out(x)\n",
    "\n",
    "        return out.view(-1,self.n_actions,self.n_quantiles)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self,maxlen):\n",
    "        self.buffer = deque(maxlen=maxlen)\n",
    "    \n",
    "    def put(self,experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self,batch_size):\n",
    "        states , actions, rewards, next_states, dones = zip(*random.sample(self.buffer,batch_size))\n",
    "        return np.stack(states), actions, rewards, np.stack(next_states), dones\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "\n",
    "class QRDQN():\n",
    "\n",
    "    def __init__(self,config):\n",
    "        self.config = config \n",
    "\n",
    "    def set_env(self,env):\n",
    "        self.env = env \n",
    "        self.initialize_agent()\n",
    "\n",
    "    def initialize_agent(self):\n",
    "\n",
    "        self.action_space = self.env.action_space  \n",
    "        self.observation = self.env.observation_space \n",
    "\n",
    "        device = self.config['device'] \n",
    "        #def __init__(self,observation_space,action_space,hidden_size = 64, num_quantiles = 16)\n",
    "\n",
    "        self.online_network = QRNetwrok(self.observation , \n",
    "                                        self.action_space, \n",
    "                                        self.config['hidden_size'], \n",
    "                                        self.config[\"n_quantiles\"]).to(device)\n",
    "        \n",
    "        self.target_network = QRNetwrok(self.observation , \n",
    "                                        self.action_space, \n",
    "                                        self.config['hidden_size'], \n",
    "                                        self.config[\"n_quantiles\"]).to(device)\n",
    "        \n",
    "        self.update_target_network(1.)\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(self.online_network.parameters() ,lr = self.config['lr'] )\n",
    "\n",
    "        self.buffer = ReplayBuffer(self.config['buffer_size'])\n",
    "\n",
    "    def update_target_network(self,tau):\n",
    "        with torch.no_grad():\n",
    "            for target_params,online_params in zip(self.target_network.parameters()\n",
    "                                                , self.online_network.parameters()):\n",
    "                target_params.data.copy_(tau*online_params.data + (1-tau)*target_params.data)\n",
    "\n",
    "    def action_selection(self , observation, eps):\n",
    "        if np.random.rand() < eps :\n",
    "            #return np.random.randint(self.action_space.n)\n",
    "            return self.env.action_space.sample()\n",
    "        else : \n",
    "            output = self.online_network.forward(torch.tensor(observation)).unsqueeze(0)# Shape: (3, 64, 64) --> (1,3,64,64)\n",
    "            return output.mean(-1).argmax().item()\n",
    "        \n",
    "    \n",
    "    def calculate_exploration_rate(self, step, total_steps):\n",
    "        epsilon_start = self.config['exploration_start_eps']\n",
    "        epsilon_end = self.config['exploration_final_eps']\n",
    "        epsilon_decay_steps = total_steps * self.config['exploration_fraction']\n",
    "        return max(epsilon_end, epsilon_start - (epsilon_start - epsilon_end) * (step / epsilon_decay_steps))\n",
    "    \n",
    "    def train(self, total_steps):\n",
    "\n",
    "        episode_count = 0 \n",
    "        episodic_reward = 0 \n",
    "        episode_rewards = [np.NaN]\n",
    "\n",
    "        observation , _ = self.env.reset()\n",
    "\n",
    "        for step in range(1,total_steps+1):\n",
    "\n",
    "            epsilon = self.calculate_exploration_rate(step,total_steps)\n",
    "            \n",
    "            action = self.action_selection(observation,epsilon)\n",
    "            next_observation, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            self.buffer.put((observation, action, reward, next_observation, done))\n",
    "\n",
    "\n",
    "            if done:\n",
    "                # Tracking\n",
    "                episode_count += 1\n",
    "                episode_rewards.append(episodic_reward)\n",
    "                episodic_reward = 0.\n",
    "                \n",
    "                # Reset environment\n",
    "                observation, _ = self.env.reset()\n",
    "            else:\n",
    "                episodic_reward += reward\n",
    "                observation = next_observation\n",
    "            \n",
    "            if step % self.config['train_frequency'] == 0 and step > self.config['learning_starts']:\n",
    "                self.learn()\n",
    "            \n",
    "            if step % self.config['target_update_interval'] == 0:\n",
    "                self.update_target_network(self.config['tau'])\n",
    "                \n",
    "            if np.mean(episode_rewards[-20:]) >= self.config['reward_target']:\n",
    "                return\n",
    "            \n",
    "            # Print training info if verbose\n",
    "            if self.config['verbose'] and step % 100 == 0:\n",
    "                print(f\"\\rStep: {step}/{total_steps} \\t Avg reward: {np.mean(episode_rewards[-20:]):.3f}\\t Episode: {episode_count}    \", end='')\n",
    "                if step % 10000 == 0:\n",
    "                    print()\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # def learn(self, kappa=1.0):\n",
    "    #     # Load batch and create tensors\n",
    "    #     states, actions, rewards, next_states, dones = self.buffer.sample(self.config['batch_size'])\n",
    "    #     states      = torch.tensor(states, dtype=torch.float32, device=self.config['device'])\n",
    "    #     actions     = torch.tensor(actions, dtype=torch.long, device=self.config['device']).view(-1, 1, 1)\n",
    "    #     rewards     = torch.tensor(rewards, dtype=torch.float32, device=self.config['device']).view(-1, 1, 1)\n",
    "    #     next_states = torch.tensor(next_states, dtype=torch.float32, device=self.config['device'])\n",
    "    #     dones       = torch.tensor(dones, dtype=torch.float32, device=self.config['device']).view(-1, 1, 1)\n",
    "        \n",
    "    #     # Get number of quantiles from config\n",
    "    #     n_quantiles = self.config['n_quantiles']\n",
    "        \n",
    "    #     # Predicted Q-value quantiles for current state\n",
    "    #     current_state_q_values = self.online_network(states)\n",
    "        \n",
    "    #     # Gather Q-value quantiles of actions actually taken\n",
    "    #     current_action_q_values = torch.gather(current_state_q_values, dim=1, index=actions.expand(-1, -1, n_quantiles))\n",
    "        \n",
    "    #     # Compute targets\n",
    "    #     with torch.no_grad():\n",
    "    #         # Get best actions in next state then gather Q-values with these actions\n",
    "    #         next_state_q_values     = self.target_network(next_states)\n",
    "    #         next_state_best_actions = torch.argmax(next_state_q_values.mean(dim=2), dim=1, keepdims=True).unsqueeze(-1)\n",
    "    #         next_state_max_q_values = torch.gather(next_state_q_values, dim=1, index=next_state_best_actions.expand(-1, -1, n_quantiles))\n",
    "            \n",
    "    #         # Bellman equation to compute target Q-values for not done states\n",
    "    #         target_q_values = rewards + self.config['gamma'] * next_state_max_q_values * (1 - dones)\n",
    "        \n",
    "    #     # Calculate TD error and Quantile Huber loss\n",
    "    #     td_error = target_q_values - current_action_q_values\n",
    "    #     huber_loss = torch.where(td_error.abs() <= kappa, \n",
    "    #                             0.5 * td_error.pow(2), \n",
    "    #                             kappa * (td_error.abs() - 0.5 * kappa))\n",
    "    #     taus = torch.linspace(0.0, 1.0, steps=n_quantiles + 2, device=self.config['device'])[1:-1].unsqueeze(0).unsqueeze(0)\n",
    "    #     quantile_loss = torch.abs(taus - (td_error < 0).float()) * huber_loss\n",
    "    #     loss = quantile_loss.mean()\n",
    "        \n",
    "    #     # Backward pass\n",
    "    #     self.optimizer.zero_grad()\n",
    "    #     loss.backward()\n",
    "    #     self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10000/400000 \t Avg reward: 21.850\t Episode: 444    \n",
      "Step: 20000/400000 \t Avg reward: 30.800\t Episode: 805    \n",
      "Step: 30000/400000 \t Avg reward: 30.550\t Episode: 1118    \n",
      "Step: 40000/400000 \t Avg reward: 29.700\t Episode: 1369    \n",
      "Step: 50000/400000 \t Avg reward: 57.050\t Episode: 1592    \n",
      "Step: 60000/400000 \t Avg reward: 52.650\t Episode: 1762    \n",
      "Step: 70000/400000 \t Avg reward: 94.950\t Episode: 1885     \n",
      "Step: 80000/400000 \t Avg reward: 70.200\t Episode: 1995     \n",
      "Step: 90000/400000 \t Avg reward: 115.150\t Episode: 2077    \n",
      "Step: 100000/400000 \t Avg reward: 156.550\t Episode: 2148    \n",
      "Step: 110000/400000 \t Avg reward: 144.650\t Episode: 2217    \n",
      "Step: 120000/400000 \t Avg reward: 155.750\t Episode: 2282    \n",
      "Step: 130000/400000 \t Avg reward: 136.800\t Episode: 2350    \n",
      "Step: 140000/400000 \t Avg reward: 145.700\t Episode: 2418    \n",
      "Step: 150000/400000 \t Avg reward: 175.700\t Episode: 2481    \n",
      "Step: 159300/400000 \t Avg reward: 192.800\t Episode: 2529    "
     ]
    }
   ],
   "source": [
    "qrdqn_config = {\n",
    "    # Device\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # Quantile regression\n",
    "    'n_quantiles'           :      9,  # For Cartpole, just use the 10th percentiles, [0.1, 0.2, ... 0.8, 0.9]\n",
    "    \n",
    "    # Network settings\n",
    "    'hidden_size'           :      8,  # Number of neurons in Q-network hidden layer\n",
    "    \n",
    "    # Buffer settings\n",
    "    'batch_size'            :     32,  # Number of experience tuples sampled per learning update\n",
    "    'buffer_size'           : 100000,  # Maximum length of replay buffer\n",
    "     \n",
    "    # Target network settings \n",
    "    'target_update_interval':   1000,  # How often to perform target network weight synchronisations\n",
    "    'tau'                   :    1.0,  # When copying online network weights to target network, what weight is given to online network weights\n",
    "     \n",
    "    # Exploration settings \n",
    "    'exploration_start_eps' :     1.,  # Initial epsilon to use\n",
    "    'exploration_final_eps' :   0.05,  # Lowest possible epsilon value\n",
    "    'exploration_fraction'  :    0.4,  # Fraction of entire training period over which the exploration rate is reduced\n",
    "     \n",
    "    # Learning settings \n",
    "    'learning_starts'       :    200,  # Step to begin learning at\n",
    "    'train_frequency'       :      3,  # Performs a learning update every `train_frequency` steps\n",
    "    'lr'                    :   1e-3,  # Learning rate\n",
    "    'gamma'                 :   0.99,  # Discount factor\n",
    "  \n",
    "    # Callback settings \n",
    "    'reward_target'         :    195,  # If set to a number, training will stop when mean reward for recent episodes exceeds this\n",
    "    'verbose'               :   True,  # Prints steps and rewards in output\n",
    "}\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "agent = QRDQN(qrdqn_config)\n",
    "agent.set_env(env)\n",
    "\n",
    "agent.train(400000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc49bab7f3878ba80ad2a65c2525cdc9d74c5f54777f705d73262dece5a900f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
