{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moha/miniconda3/envs/gym_env/lib/python3.8/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise(object):\n",
    "    def __init__(self, mu, sigma = 0.15, theta = 0.2, dt = 1e-2 , x0 = None):\n",
    "        self.theta = theta \n",
    "        self.mu = mu\n",
    "        self.dt = dt \n",
    "        self.sigma = sigma \n",
    "        self.x0 = x0 \n",
    "        self.reset()\n",
    "    \n",
    "    def __call__(self):\n",
    "        # Noise = OUActionNoise()\n",
    "        # ournoise = Noise() --> This is why __call__() is used.\n",
    "        x = self.x_prev + self.theta*(self.mu - self.x_prev)*self.dt +\\\n",
    "            self.sigma*np.sqrt(self.dt)*np.random.normal(size = self.mu.shape)\n",
    "        self.x_prev = x \n",
    "        return x \n",
    "    \n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "    \n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size \n",
    "        self.mem_cntr = 0 \n",
    "        self.state_memory = np.zeros((self.mem_size * input_shape))\n",
    "        self.new_state_memory = np.zeros((self.mem_size * input_shape))\n",
    "        self.action_memory = np.zeros((self.mem_size * n_actions))\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size , dtype = np.float32)\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = next_state\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = 1 - int(done)\n",
    "        self.mem_cntr += 1 \n",
    "    \n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr , self.mem_size)\n",
    "        batch = np.random.choice(max_mem , batch_size)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        new_states = self.new_state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, new_states, terminal\n",
    "\n",
    "\n",
    "class Actor(object):\n",
    "    def __init__(self , lr , n_actions, name, input_dims, fc1_dims,fc2_dims, action_bound\n",
    "                , batch_size = 64, chkpt_dir = 'tmp/ddpg'):\n",
    "        self.lr = lr \n",
    "        self.n_actions = n_actions \n",
    "        self.name = name \n",
    "        self.fc1_dims = fc1_dims \n",
    "        self.fc2_dims = fc2_dims \n",
    "        self.batch_size = batch_size\n",
    "        self.action_bound = action_bound \n",
    "        self.chkpt_dir = chkpt_dir\n",
    "        self.build_network()\n",
    "        self.params = tf.trainable_varaibles(scope = self.name)\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir , name+'_ddpg,ckpt')\n",
    "\n",
    "\n",
    "        self.unnormalized_actor_gradients = tf.gradients(self.mu , self.params, -self.action_gradients)\n",
    "\n",
    "        self.actor_gradients = list(map(lambda x:tf.div(x,self.batch_size), self.unnormalized_actor_gradients))\n",
    "        self.optimize = tf.train.AdamOptimizer(self.lr).apply_gradients(zip(self.actor_gradients , self.params))\n",
    "\n",
    "    def build_network(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.input = tf.placeholder(tf.float32 , shape = [None, *self.input_dims] , name = 'inputs')\n",
    "            self.action_gradient = tf.placeholder\n",
    "\n",
    "     \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc49bab7f3878ba80ad2a65c2525cdc9d74c5f54777f705d73262dece5a900f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
