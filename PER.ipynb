{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro \n",
    "In this notebook, we will be looking at an extension to the DQN-based model that we have been using so far. This extension is called Prioritized Experience Replay (PER). Main paper for this repo is [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952).\n",
    "\n",
    "The general idea is to  sample experiences from the replay buffer in a non-uniform way. Instead of sampling experiences uniformly at random, we will sample experiences with a probability that is proportional to the absolute TD error. This way, we will be more likely to sample experiences that are more surprising or unexpected. This can help the agent learn faster and more effectively.\n",
    "\n",
    "As discussed previously, TD error is defined as:\n",
    "\n",
    "$$\n",
    "\\delta = r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
