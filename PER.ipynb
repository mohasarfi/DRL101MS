{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro \n",
    "In this notebook, we will be looking at an extension to the DQN-based model that we have been using so far. This extension is called Prioritized Experience Replay (PER). Main paper for PER is [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952).\n",
    "\n",
    "The general idea is to  sample experiences from the replay buffer in a non-uniform way. Instead of sampling experiences uniformly at random, we will sample experiences with a probability that is proportional to the absolute TD error. This way, we will be more likely to sample experiences that are more surprising or unexpected. This can help the agent learn faster and more effectively.\n",
    "\n",
    "As discussed previously, TD error is defined as:\n",
    "\n",
    "$$\n",
    "\\delta = r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.9.0+750d7f9)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "{'lives': 0, 'episode_frame_number': 291, 'frame_number': 4026}\n",
      "(84, 84)\n"
     ]
    }
   ],
   "source": [
    "# Game of Pong Simulation environment\n",
    "import gymnasium as gym\n",
    "import gymnasium.utils.seeding as seeding\n",
    "from gymnasium.wrappers import AtariPreprocessing, RecordVideo\n",
    "import ale_py\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "DefaultRandomSeed = 10 \n",
    "# Create the Pong environment\n",
    "env = gym.make(\"ALE/Pong-v5\",frameskip=1)\n",
    "env.np_random, _ = seeding.np_random(DefaultRandomSeed)\n",
    "env.reset(seed=DefaultRandomSeed)\n",
    "env = AtariPreprocessing(env) # Frame skipping, Grayscale, Resize (To 84*84), Stack 4 frames\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example interaction with the environment\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()  # Take a random action\n",
    "    observation, reward , terminated, truncated, info = env.step(action)  # Apply the action\n",
    "    \n",
    "\n",
    "    if terminated or truncated:\n",
    "        state = env.reset()  # Reset the environment if done\n",
    "\n",
    "print(terminated)\n",
    "print (truncated)\n",
    "print(info)\n",
    "print(observation.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PER Algorithm walkthrough\n",
    "\n",
    "This section provides a high-level overview of the PER algorithm. The algorithm is based on the DQN algorithm, with some modifications to the replay buffer and sampling process. The following steps are written based on the the pseduo code provided in the paper.\n",
    "\n",
    "1. Initialize Replay Buffer ($\\mathcal{H}$) with capacity $N$.\n",
    "2. In each episode\n",
    "    -  In each step (Until episode is done or terminated)\n",
    "        - Run action selection policy\n",
    "        - Store transition ($s_t$, $a_t$, $r_{t+1}$, $s_{t+1}$, $p_{t}$) in $\\mathcal{H}$\n",
    "        - for each transition in mininbatch (1:$\\mathcal{K}$)\n",
    "            - Sample transition j with the probability of $P({j}) = \\frac{p_{j}^{\\alpha}}{\\sum_{i}p_{i}^{\\alpha}}$\n",
    "            - Compute the importance sampling weight $w_{j} = \\left( \\frac{1}{N} \\cdot \\frac{1}{P(j)} \\right)^{\\beta}$\n",
    "            - Compute the TD error $\\delta_{j} = r_{j} + \\gamma \\max_{a^{\\prime}} Q(s_{j+1},a^{\\prime}, \\theta) - Q(s_{j},a_{j}, \\theta)$ (TD error deponds on the type of DQN algorithm)\n",
    "            - update the priority of transition $j$ in $\\mathcal{H}$ as $p_{j} = |\\delta_{j}| + \\epsilon$\n",
    "            - Accumulate the weight change $\\Delta \\ = \\Delta  + w_{j} \\delta_{j} \\nabla_{\\theta} Q(s_{j},a_{j}, \\theta)$\n",
    "        - Update the Q network weights using the accumulated weight change $\\theta = \\theta + \\eta \\Delta$\n",
    "        - - every C steps update $\\hat{Q}$ network weights using the following equation\n",
    "            $\\bar{\\theta} = \\tau*\\theta + (1 - \\tau)*\\bar{\\theta}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 84, 84)\n",
      "QNetwork(\n",
      "  (conv1): Conv2d(2, 16, kernel_size=(8, 8), stride=(4, 4))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2))\n",
      "  (fc1): Linear(in_features=2592, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions,seed):\n",
    "        super(QNetwork, self).__init__()\n",
    "        print(input_shape)\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_shape[0], out_channels=16, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2)\n",
    "        self.fc1 = nn.Linear(in_features=self._feature_size(input_shape), out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=num_actions)\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input : Observations \n",
    "        # Ouput : Q value of different actions\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def _feature_size(self, input_shape):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 16, 8, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 4, 2),\n",
    "            nn.ReLU()\n",
    "        ).forward(torch.zeros(1, *input_shape)).view(1, -1).size(1)\n",
    "\n",
    "net = QNetwork((2, 84, 84), 4,10) \n",
    "print(net)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prioritized_ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size, seed,alpha=0.6,beta=0.4,beta_increment=1e-5):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.memory = deque(maxlen=self.buffer_size)\n",
    "        self.priorities = deque(maxlen=self.buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.priorities = namedtuple(\"Priorities\", field_names=[\"priority\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        self._alpha = alpha # alpha is the importance sampling factor. alpha = 0 means uniform sampling\n",
    "        self._beta = beta\n",
    "        self._max_priority = 1.0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "        self.priorities.append(self._max_priority)\n",
    "\n",
    "    def sample(self):\n",
    "        # Sample the batch\n",
    "        probs = np.array(self.priorities**self._alpha)/np.sum(self.priorities**self._alpha)\n",
    "        idx = np.random.choice(np.arange(len(self.memory)), size=self.batch_size, p=probs, replace=False)\n",
    "        experiences = [self.memory[i] for i in idx]\n",
    "        weights = (self.buffer_size*probs[idx])**(-self._beta)\n",
    "        weights = weights/np.max(weights)\n",
    "        return experiences, idx , weights\n",
    "    \n",
    "    def update_priority(self, idx, priority):\n",
    "        self.priorities[idx] = priority + 1e-5\n",
    "        self._max_priority = max(self._max_priority, priority)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, state_size, action_size, seed ,random_policy=False,beta=0.4):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.random = random_policy\n",
    "        self.beta = beta\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = Prioritized_ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def load_weights(self, model_weights):\n",
    "        self.qnetwork_local.load_state_dict(torch.load('models/{}'.format(model_weights)))\n",
    "    \n",
    "    def save_weights(self, model_weights):\n",
    "        torch.save(self.qnetwork_local.state_dict(), 'models/{}'.format(model_weights))\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        if self.random:\n",
    "            return\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences, idx,weights = self.memory.sample()\n",
    "                self.learn(experiences, idx,weights,GAMMA)\n",
    "    \n",
    "    def act(self, state, eps=0.):\n",
    "        if self.random:\n",
    "            return np.random.randint(self.action_size)\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "    \n",
    "    def learn(self, experiences, idx,weights, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        current = self.qnetwork_local(states).gather(1, actions)\n",
    "        next_qvalues = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        target = rewards + (gamma * next_qvalues * (1 - dones))\n",
    "        # calculate the TD error\n",
    "        td_error = target - current\n",
    "        # update the priority\n",
    "        for i in range(len(idx)):\n",
    "            self.memory.update_priority(idx[i], td_error[i].item())\n",
    "        \n",
    "        loss = (td_error* weights).pow(2)   \n",
    "\n",
    "        self.qnetwork_local.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
    "\n",
    "        def soft_update(self, local_model, target_model, tau):\n",
    "            for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "                target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Training():\n",
    "    def __init__(self, env, agent, n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_decay = eps_decay\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        for ep_number in range(1, self.n_episodes+1):\n",
    "            state,_ = self.env.reset()\n",
    "            score = 0\n",
    "            eps = max(self.eps_end, self.eps_start*self.eps_decay)\n",
    "            episode_step = 0\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                episode_step += 1\n",
    "                action = self.agent.act(state, eps)\n",
    "                next_state, reward, done, terminated, truncated, info = self.env.step(action)\n",
    "                if terminated or truncated or episode_step >= self.max_t:\n",
    "                    done = True\n",
    "                self.agent.step(state, action, reward, next_state, done)\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "            print('Episode: {}\\tScore: {:.2f}'.format(ep_number, score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
