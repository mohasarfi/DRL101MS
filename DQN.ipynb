{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "While this notebook focuses on understanding through clear explanations and visualizations,a professional implementation could involve separate files for better organization:\n",
    "- **model.py**: Contains the neural network architecture.\n",
    "- **agent.py**: Contains the DRL algorithm.\n",
    "- **trainning.py**: Contains the agent interaction with the environment to provide the training and testing loops.\n",
    "\n",
    "This notebook contains all above files in one place for better understanding! Consider this notebook the first chapter of our DRL adventure! In next notebooks, we'll be more focused on the implementation of the algorithms. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.9.0+750d7f9)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "{'lives': 0, 'episode_frame_number': 295, 'frame_number': 4026}\n"
     ]
    }
   ],
   "source": [
    "# Game of Pong Simulation environment\n",
    "import gymnasium as gym\n",
    "import gymnasium.utils.seeding as seeding\n",
    "from gymnasium.wrappers import AtariPreprocessing, RecordVideo\n",
    "import ale_py\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "DefaultRandomSeed = 10 \n",
    "# Create the Pong environment\n",
    "env = gym.make(\"ALE/Pong-v5\",frameskip=1)\n",
    "env.np_random, _ = seeding.np_random(DefaultRandomSeed)\n",
    "env.reset(seed=DefaultRandomSeed)\n",
    "env = AtariPreprocessing(env) # Frame skipping, Grayscale, Resize (To 84*84), Stack 4 frames\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example interaction with the environment\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()  # Take a random action\n",
    "    observation, reward , terminated, truncated, info = env.step(action)  # Apply the action\n",
    "\n",
    "    if terminated or truncated:\n",
    "        state = env.reset()  # Reset the environment if done\n",
    "\n",
    "print(terminated)\n",
    "print(info)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Algorithm walk through \n",
    "In this part we will review all the steps in the DQN in general and in the next sections, we will learn about each step in details. The following steps are written based on the pseducode providede in the [main DQN paper](https://arxiv.org/abs/1312.5602). Steps are as folllows :\n",
    "1. Initialize Experince Replay Buffer ($\\mathcal{D} $)\n",
    "2. Initialize action-value function network (${Q}$) with random weights ($\\theta$)\n",
    "3. Initialize target action-value function network ($\\hat{Q}$)\n",
    "4. In each episode\n",
    "    -  In each step (Until episode is done or terminated)\n",
    "        - Run action selection policy\n",
    "            - chose a random action with a probability of $\\epsilon$\n",
    "            - **Otherwise** use action that maximizes the output of ${Q}$ network\n",
    "        - step function! : take action $a_t$, shift to $s_{t+1}$, and receive reward $r_{t}$\n",
    "        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2,  4, kernel_size=6, stride=2)  # 2x84x84 to 4x40x40\n",
    "        self.conv2 = nn.Conv2d(4, 16, kernel_size=8, stride=4)  # 4x40x40 to 16x9x9\n",
    "        self.lsize = 16*9*9\n",
    "        self.fc1 = nn.Linear(self.lsize, 256)\n",
    "        self.fc2 = nn.Linear(256, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input : Observations \n",
    "        # Ouput : Q value of different actions\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, self.lsize)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Experience Replay Buffer "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of (state, action, reward, next state) is saved in the experience replay buffer at each step, and a training sample is selected randomly from it. Experience Replay Buffer is used to solve the following issues \n",
    "1. More Diverse Mini-Batches for Training\n",
    "2. Reduces Overfitting to Recent Experiences\n",
    "3. Mitigating the Non-i.i.d. Issue (independent and identically distributed samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.memory.append(self.experience(state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the next block, we have defined two networks with the same stracture known as \"Local\" and \"Target\" Q networks. But why?!\n",
    "The **non-stationary** behavior of the DRL methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DQNAgent():\n",
    "    def __init__(self, state_size, action_size, seed, random_policy=False):\n",
    "        self.random = random_policy\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def load_weights(self, model_weights):\n",
    "        self.qnetwork_local.load_state_dict(torch.load('models/{}'.format(model_weights)))\n",
    "    \n",
    "    def save_weights(self, model_weights):\n",
    "        torch.save(self.qnetwork_local.state_dict(), 'models/{}'.format(model_weights))\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        if self.random:\n",
    "            return\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        if self.random:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device) # unsqueeze adds an extra dimension to the provided tensor\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        current = self.qnetwork_local(states).gather(1, actions) # return Q(s,a)\n",
    "        next_qvalues = self.qnetwork_target(next_states).max(1)[0].detach().unsqueeze(1)\n",
    "        targets = rewards + GAMMA*(next_qvalues*(1 - dones))\n",
    "        loss = F.smooth_l1_loss(current, targets)\n",
    "\n",
    "        self.qnetwork_local.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be added ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc49bab7f3878ba80ad2a65c2525cdc9d74c5f54777f705d73262dece5a900f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
