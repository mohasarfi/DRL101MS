{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "While this notebook focuses on understanding through clear explanations and visualizations,a professional implementation could involve separate files for better organization:\n",
    "- **model.py**: Contains the neural network architecture.\n",
    "- **agent.py**: Contains the DRL algorithm.\n",
    "- **trainning.py**: Contains the agent interaction with the environment to provide the training and testing loops.\n",
    "\n",
    "This notebook contains all above files in one place for better understanding! Consider this notebook the first chapter of our DRL adventure! In next notebooks, we'll be more focused on the implementation of the algorithms. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "{'lives': 0, 'episode_frame_number': 4000, 'frame_number': 4000}\n",
      "(84, 84)\n"
     ]
    }
   ],
   "source": [
    "# Game of Pong Simulation environment\n",
    "import gymnasium as gym\n",
    "import gymnasium.utils.seeding as seeding\n",
    "from gymnasium.wrappers import AtariPreprocessing, RecordVideo\n",
    "import ale_py\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "DefaultRandomSeed = 10 \n",
    "# Create the Pong environment\n",
    "env = gym.make(\"ALE/Pong-v5\",frameskip=1)\n",
    "env.np_random, _ = seeding.np_random(DefaultRandomSeed)\n",
    "env.reset(seed=DefaultRandomSeed)\n",
    "env = AtariPreprocessing(env) # Frame skipping, Grayscale, Resize (To 84*84), Stack 4 frames\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example interaction with the environment\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()  # Take a random action\n",
    "    observation, reward , terminated, truncated, info = env.step(action)  # Apply the action\n",
    "    \n",
    "\n",
    "    if terminated or truncated:\n",
    "        state = env.reset()  # Reset the environment if done\n",
    "\n",
    "print(terminated)\n",
    "print (truncated)\n",
    "print(info)\n",
    "print(observation.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Algorithm walkthrough \n",
    "\n",
    "This section provides a high-level overview of the Deep Q-Network (DQN) algorithm. We will learn about each step in details. The following steps are written based on the pseducode providede in the [main DQN paper](https://arxiv.org/abs/1312.5602).\n",
    "\n",
    "1. Initialize Experince Replay Buffer ($\\mathcal{D} $)\n",
    "2. Initialize action-value function network (${Q}$) with random weights ($\\theta$)\n",
    "3. Initialize target action-value function network ($\\hat{Q}$)\n",
    "4. In each episode\n",
    "    -  In each step (Until episode is done or terminated)\n",
    "        - Run action selection policy\n",
    "            - chose a random action with a probability of $\\epsilon$\n",
    "            - **Otherwise** use action that maximizes the output of ${Q}$ network\n",
    "        - step function! : take action $a_t$, shift to $s_{t+1}$, and receive reward $r_{t}$\n",
    "        - store ($s_{t}$,$a_{t}$,$r_{t}$,$s_{t+1}$) in $\\mathcal{D} $\n",
    "        - Sample mininbatch of transitions from $\\mathcal{D} $\n",
    "        - Calculate target value:\n",
    "            - if episode is done or terminated:\n",
    "                $y_{j} = r_{j}$\n",
    "            - Otherwise:\n",
    "                $y_{j} = r_{j} + \\gamma \\text{max}_{a^{\\prime}} \\hat{Q}(s_{j+1},a^{\\prime}, \\bar{\\theta})$\n",
    "        - Define loss as  $(y_{j} - Q(s_{j},a_{j}, \\theta))$\n",
    "        - Update ${Q}$ weights given the provided loss \n",
    "        - every C steps update $\\hat{Q}$ network weights using the following equation\n",
    "            $\\bar{\\theta} = \\tau*\\theta + (1 - \\tau)*\\bar{\\theta}$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model \n",
    "No matter what DRL algorithm we use, the way the deep neural network structure is super important in decision-making process. We're starting with a basic type of network known as Convolutional Neural Network (CNN), but as we go further, we will try networks with more complex structures!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 84, 84)\n",
      "QNetwork(\n",
      "  (conv1): Conv2d(2, 16, kernel_size=(8, 8), stride=(4, 4))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2))\n",
      "  (fc1): Linear(in_features=2592, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions,seed):\n",
    "        super(QNetwork, self).__init__()\n",
    "        print(input_shape)\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_shape[0], out_channels=16, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2)\n",
    "        self.fc1 = nn.Linear(in_features=self._feature_size(input_shape), out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=num_actions)\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input : Observations \n",
    "        # Ouput : Q value of different actions\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def _feature_size(self, input_shape):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 16, 8, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 4, 2),\n",
    "            nn.ReLU()\n",
    "        ).forward(torch.zeros(1, *input_shape)).view(1, -1).size(1)\n",
    "\n",
    "net = QNetwork((2, 84, 84), 4,10) \n",
    "print(net)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Experience Replay Buffer "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of (state, action, reward, next state) is saved in the experience replay buffer at each step, and a training sample is selected randomly from it. Experience Replay Buffer is used to solve the following issues \n",
    "1. More Diverse Mini-Batches for Training\n",
    "2. Reduces Overfitting to Recent Experiences\n",
    "3. Mitigating the Non-i.i.d. Issue (independent and identically distributed samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.memory.append(self.experience(state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Network\n",
    "As you'll see in the next section, we define two networks with the same structure: a \"Local\" network and a \"Target\" network. Why do we need both?\n",
    "\n",
    "The problem lies in the constantly changing behavior (non-stationary) of DRL methods. The targets we use to train our network are calculated using the network itself. This means the target function changes with every update, making the learning process unstable.\n",
    "\n",
    "A simple yet effective solution is to have a separate network, called the target network, that we fix for multiple training steps. This network remains unchanged while the local network updates. We use the target network to calculate more stable targets for training the local network (step 3 in the walkthrough).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DQNAgent():\n",
    "    def __init__(self, state_size, action_size, seed, random_policy=False):\n",
    "        self.random = random_policy\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def load_weights(self, model_weights):\n",
    "        self.qnetwork_local.load_state_dict(torch.load('models/{}'.format(model_weights)))\n",
    "    \n",
    "    def save_weights(self, model_weights):\n",
    "        torch.save(self.qnetwork_local.state_dict(), 'models/{}'.format(model_weights))\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        if self.random:\n",
    "            return\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        if self.random:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device) # unsqueeze adds an extra dimension to the provided tensor\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        current = self.qnetwork_local(states).gather(1, actions) # return Q(s,a)\n",
    "        next_qvalues = self.qnetwork_target(next_states).max(1)[0].detach().unsqueeze(1)\n",
    "        targets = rewards + GAMMA*(next_qvalues*(1 - dones))\n",
    "        loss = F.smooth_l1_loss(current, targets)\n",
    "\n",
    "        self.qnetwork_local.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Training():\n",
    "    def __init__(self, n_episodes=2000, eps_start=1.0, eps_end=0.01, eps_decay=0.995, max_t=1000):\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps_end = eps_end\n",
    "        self.eps = self.eps_start\n",
    "    \n",
    "    def train(self,agent,env):\n",
    "\n",
    "        for ep_number in range(self.n_episodes+1):\n",
    "            state, _ = env.reset()\n",
    "            print(state.shape)\n",
    "            episode_reward, done = 0, False\n",
    "\n",
    "            episode_step = 0\n",
    "\n",
    "\n",
    "            while not done:\n",
    "                episode_step += 1 \n",
    "                action = agent.act(state, self.eps)\n",
    "                next_state, reward , terminated, truncated, info = env.step(action)  # Apply the action\n",
    "                if terminated or truncated :\n",
    "                    done = True \n",
    "\n",
    "                agent.step(state, action, reward, next_state, done)\n",
    "                \n",
    "                episode_reward += reward \n",
    "                state = next_state\n",
    "            \n",
    "            self.eps = max(self.eps_end, self.eps_decay*self.eps)\n",
    "\n",
    "            if ep_number % 10 == 0 :\n",
    "                print(episode_reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc49bab7f3878ba80ad2a65c2525cdc9d74c5f54777f705d73262dece5a900f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
